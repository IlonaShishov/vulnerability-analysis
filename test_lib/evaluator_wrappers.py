import os
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA
    
def wrap_evaluator_embeddings():

    nvidia_embeddings_params = {
        "base_url": os.getenv("EMBEDDING_MODEL_BASE_URL") or None,
        "model": os.getenv("EMBEDDING_MODEL_NAME") or None,
        "api_key": os.getenv("API_KEY") or None,
        "truncate": os.getenv("TRUNCATE") or None,
        "max_batch_size": int(os.getenv("MAX_BATCH_SIZE")) if os.getenv("MAX_BATCH_SIZE") else None,
        "dimensions": int(os.getenv("DIMENSIONS")) if os.getenv("DIMENSIONS") else None
    }

    clean_nvidia_embeddings_params = {param: value for param, value in nvidia_embeddings_params.items() if value is not None}

    if not ("base_url" in clean_nvidia_embeddings_params or "model" in clean_nvidia_embeddings_params):
        raise ValueError("The Embeddings configuration must contain at least 'base_url' or 'model'.")

    return LangchainEmbeddingsWrapper(NVIDIAEmbeddings(**clean_nvidia_embeddings_params))

def wrap_evaluator_llm():

    chat_nvidia_params = {
        "base_url": os.getenv("LLM_MODEL_BASE_URL") or None,
        "model": os.getenv("LLM_MODEL_NAME") or None,
        "api_key": os.getenv("API_KEY") or None,
        "temperature": float(os.getenv("TEMPERATURE")) if os.getenv("TEMPERATURE") else None,
        "top_p": float(os.getenv("TOP_P")) if os.getenv("TOP_P") else None,
        "max_tokens": int(os.getenv("MAX_TOKENS")) if os.getenv("MAX_TOKENS") else None,
    }

    clean_chat_nvidia_params = {param: value for param, value in chat_nvidia_params.items() if value is not None}

    if not ("base_url" in clean_chat_nvidia_params or "model" in clean_chat_nvidia_params):
        raise ValueError("The LLM configuration must contain at least 'base_url' or 'model'.")

    return LangchainLLMWrapper(ChatNVIDIA(**clean_chat_nvidia_params))
