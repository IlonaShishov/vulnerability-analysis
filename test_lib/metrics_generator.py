from openai import OpenAI
from ragas.metrics import (
    SemanticSimilarity, 
    LLMContextPrecisionWithoutReference, 
    LLMContextRecall, 
    ResponseRelevancy, 
    Faithfulness, 
    AspectCritic, 
    SummarizationScore
)
import pandas as pd

from test_lib.evaluator_wrappers import wrap_evaluator_embeddings, wrap_evaluator_llm

class MetricsGenerator:

    def __init__(self, data, metrics_request):
        self.data = data
        self.metrics_table = pd.DataFrame({'ID': list(data.keys())})

        self.single_turn_samples = metrics_request.get_single_turn_samples(self.data)
        
        self.wrapped_evaluator_embeddings = wrap_evaluator_embeddings()
        self.wrapped_evaluator_llm = wrap_evaluator_llm()
        
        self.metrics = metrics_request.metrics

    async def _score_samples(self, scorer, samples):
        for case_id, samples in samples.items():
            total_score = 0
            for sample in samples:
                score = await scorer.single_turn_ascore(sample)
                total_score += score

            avg_score = total_score / len(samples) if len(samples) > 0 else 0
            self.metrics_table.loc[self.metrics_table['ID'] == case_id, scorer.name] = avg_score


    async def evaluate_metric(self, scorer_class, sample_type, **scorer_args):
        scorer = scorer_class(**scorer_args)
        samples = getattr(self, sample_type)
        await self._score_samples(scorer, samples)


    def _build_scorer_args(self, metric_info):
        scorer_args = {'name': metric_info['name'], **metric_info.get('kwargs', {})}
        
        if metric_info.get('llm', False):
            scorer_args['llm'] = self.wrapped_evaluator_llm
        
        if metric_info.get('embeddings', False):
            scorer_args['embeddings'] = self.wrapped_evaluator_embeddings  

        return scorer_args      

    async def __getattr__(self, attr):
        """
        Dynamically run the metric based on attribute access.
        """
        if attr in self.metrics:
            metric_info = self.metrics[attr]

            scorer_args = self._build_scorer_args(metric_info)

            await self.evaluate_metric(
                scorer_class=globals().get(metric_info["scorer_class"]),
                sample_type=metric_info["sample_type"],
                **scorer_args
            )
        else:
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{attr}'")