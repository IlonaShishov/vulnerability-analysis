from openai import OpenAI
from ragas.metrics import (
    SemanticSimilarity, 
    LLMContextPrecisionWithoutReference, 
    LLMContextRecall, 
    ResponseRelevancy, 
    Faithfulness, 
    AspectCritic, 
    SummarizationScore
)
from ragas import EvaluationDataset, evaluate
import pandas as pd

from test_lib.evaluator_wrappers import wrap_evaluator_embeddings, wrap_evaluator_llm

class MetricsGenerator:

    def __init__(self, data, metrics_request):
        self.data = data
        self.metrics_table = pd.DataFrame({'ID': list(data.keys())})

        self.datasets = metrics_request.get_datasets(self.data)
        self.metrics = [globals().get(name)() for name in metrics_request.metrics if globals().get(name) is not None]

        self.wrapped_evaluator_embeddings = wrap_evaluator_embeddings()
        self.wrapped_evaluator_llm = wrap_evaluator_llm()

        self.evaluate_datasets()


    def evaluate_datasets(self):
        for case_id, dataset in self.datasets.items():
            evaluation_dataset = EvaluationDataset.from_list(dataset)            
            results = evaluate(
                dataset = evaluation_dataset,
                metrics=self.metrics,
                llm = self.wrapped_evaluator_llm,
                embeddings = self.wrapped_evaluator_embeddings,
                # run_config
                # batch_size
            )
            scores = results.scores[0]
            self.metrics_table.loc[self.metrics_table['ID'] == case_id, scores.keys()] = scores.values()