# Integration Tests for Agent Morpheus

This repository contains integration tests for **Agent Morpheus Vulnerability Analysis**, designed to generate metrics for the agent's responses (outputs). The integration tests utilize the `test_lib` library, which encapsulates all the metric logic required for testing.

## Prerequisites

### 1. Install the Environment
Ensure that the Conda environment is set up correctly using the provided `environment.yaml` file:

```bash
conda env create -f environment.yaml
conda activate <your_env_name>
```

### 2. Create an `examples` Folder

Create an `./examples` folder in the root directory of the project. Place the desired **Agent Morpheus** output files in JSON format into this folder. These files will be used to generate the metrics.

### 3. Create a `.env` File

Create a `.env` file in the root folder of the project with the following information:

```
# .env

EXAMPLES_DIR="./examples"
EXAMPLES_XLSX_FILE_NAME="agent_morpheus_metrics.xlsx"

EMBEDDING_MODEL_BASE_URL=
EMBEDDING_MODEL_NAME=

LLM_MODEL_NAME=
API_KEY=
TEMPERATURE=
TOP_P=
MAX_TOKENS=
```

Populate the placeholders (`EMBEDDING_MODEL_BASE_URL`, `EMBEDDING_MODEL_NAME`, etc.) with the appropriate values.

## Running the Integration Tests
To run the metrics generation, execute the following command in the root folder:

```bash
pytest -s tests/integration/generate_metrics.py
```

## Adding Metrics
To extend the functionality of the integration tests by adding new metrics, follow these steps:

### 1. Adding Metrics to Summary Metrics
To add a new metric to the **Summary Metrics**, update the `self.metrics` list in the following file:

`test_lib/summary_metrics.py`

Example:
```python
self.metrics = {
            "SemanticSimilariy": {},
            ...
            "NewMetricClass": {}
        }
```

### 2. Adding Metrics to Justification Label Metrics
To add a new metric to the **Justification Label Metrics**, update the `self.metrics` list in the following file:

`test_lib/justification_label_metrics.py`

Example:
```python
self.metrics = {
            "Faithfulness": {},
            ...
            "NewMetricClass": {}
        }
```

### 3. Import the Metric in `metrics_generator.py`
Ensure that the new metric class is properly imported in the following file:

`test_lib/metrics_generator.py`

Example:
```python
from ragas.metrics import (
    SemanticSimilarity, 
    ...
    NewMetricClass
)
```

By following these steps, the new metric will be seamlessly integrated into the testing framework.


