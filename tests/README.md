# Integration Tests for Agent Morpheus

This repository contains integration tests for **Agent Morpheus Vulnerability Analysis**, designed to generate metrics for the agent's responses (outputs). The integration tests utilize the `test_lib` library, which encapsulates all the metric logic required for testing.

## Prerequisites

### 1. Install the Environment
Ensure that the Conda environment is set up correctly using the provided `environment.yaml` file:

```bash
conda env create -f environment.yaml
conda activate <your_env_name>
```

### 2. Create an `examples` Folder

Create an `./examples` folder in the root directory of the project. Place the desired **Agent Morpheus** output files in JSON format into this folder. These files will be used to generate the metrics.

### 3. Create a `.env` File

Create a `.env` file in the root folder of the project with the following information:

```
# .env

EXAMPLES_DIR="./examples"
EXAMPLES_XLSX_FILE_NAME="agent_morpheus_metrics.xlsx"

API_KEY="your-api-key" # optional depending on self-hosted config

EMBEDDING_MODEL_BASE_URL="http://self-hosted-embeddings-base-url/v1" # optional
EMBEDDING_MODEL_NAME="your-model"
TRUNCATE="NONE"
MAX_BATCH_SIZE="50"
DIMENSIONS="256"

LLM_MODEL_BASE_URL="http://self-hosted-model-base-url/v1" # optional
LLM_MODEL_NAME="your-model"
TEMPERATURE="0.2"
TOP_P="0.7"
MAX_TOKENS="1024"
```

Populate the placeholders (`EXAMPLES_DIR`, `EXAMPLES_XLSX_FILE_NAME`, etc.) with the appropriate values.

## Running the Integration Tests
To run the metrics generation, execute the following command in the root folder:

```bash
pytest -s tests/integration/generate_metrics.py
```

## Adding Metrics
To extend the functionality of the integration tests by adding new metrics, follow these steps:

### 1. Adding Metrics to an Evaluation Report
To add a new metric to an evaluation report, navigate to the desired report in **test_lib/reports** and modify the `self.metrics` parameter.

`test_lib/reports/<report>_metrics.py`

Example:
```python
self.metrics = {
            "SemanticSimilarity": {},
            ...
            "NewMetricClass": {}
        }
```

### 2. Import the Metric in `metrics_generator.py`
Ensure that the new metric class is properly imported in the following file:

`test_lib/metrics_generator.py`

Example:
```python
from ragas.metrics import (
    SemanticSimilarity, 
    ...
    NewMetricClass
)
```

By following these steps, the new metric will be seamlessly integrated into the testing framework.


