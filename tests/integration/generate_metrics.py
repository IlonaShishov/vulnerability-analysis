import pytest
import pandas as pd
import os

from metrics_lib.data_loader import load_test_data
from metrics_lib.reports.summary import SummaryMetrics
from metrics_lib.reports.justification_label import JustificationLabelMetrics, VulnerabilityConsistencyReport, JustificationLabelConsistencyReport
from metrics_lib.reports.checklist_response import ChecklistResponseMetrics
from metrics_lib.reports.checklist_question import ChecklistQuestionMetrics
from metrics_lib.metrics_generator import MetricsGenerator
from metrics_lib.metrics_visualizer import MetricsVisualizer
from metrics_lib.display.excel import save_to_excel
from metrics_lib.display.console import print_to_console

@pytest.fixture
def test_data():
    examples_dir = os.getenv("EXAMPLES_DIR")
    return load_test_data(examples_dir)

def test_consistency(test_data):

    vulnerability_consistency_report = VulnerabilityConsistencyReport(test_data)
    print_to_console(vulnerability_consistency_report.report_table, vulnerability_consistency_report.name)
    save_to_excel(vulnerability_consistency_report.report_table, os.getenv("EXAMPLES_XLSX_FILE_NAME"), vulnerability_consistency_report.name)

    justification_label_consistency_report = JustificationLabelConsistencyReport(test_data)    
    print_to_console(justification_label_consistency_report.report_table, justification_label_consistency_report.name)
    save_to_excel(justification_label_consistency_report.report_table, os.getenv("EXAMPLES_XLSX_FILE_NAME"), justification_label_consistency_report.name)

@pytest.mark.asyncio
async def test_justification_label_metrics(test_data):

    print()
    report = JustificationLabelMetrics()
    metrics_generator = MetricsGenerator(test_data, report)
    await metrics_generator.evaluate_datasets()
    metrics_visualizer = MetricsVisualizer(metrics_generator.metrics_table, report.name)
    metrics_visualizer.plot_heatmap()

    print_to_console(metrics_generator.metrics_table, report.name, report.metrics)
    save_to_excel(metrics_generator.metrics_table, os.getenv("EXAMPLES_XLSX_FILE_NAME"), report.name, False, report.metrics)

@pytest.mark.asyncio
async def test_summary_metrics(test_data):

    print()
    report = SummaryMetrics()
    metrics_generator = MetricsGenerator(test_data, report)
    metrics_visualizer = MetricsVisualizer(metrics_generator.metrics_table, report.name)
    metrics_visualizer.plot_heatmap()

    print_to_console(metrics_generator.metrics_table, report.name, report.metrics)
    save_to_excel(metrics_generator.metrics_table, os.getenv("EXAMPLES_XLSX_FILE_NAME"), report.name, False, report.metrics)

@pytest.mark.asyncio
async def test_checklist_response_metrics(test_data):

    print()
    report = ChecklistResponseMetrics()
    metrics_generator = MetricsGenerator(test_data, report)
    metrics_visualizer = MetricsVisualizer(metrics_generator.metrics_table, report.name)
    metrics_visualizer.plot_heatmap()

    print_to_console(metrics_generator.metrics_table, report.name, report.metrics)
    save_to_excel(metrics_generator.metrics_table, os.getenv("EXAMPLES_XLSX_FILE_NAME"), report.name, False, report.metrics)

@pytest.mark.asyncio
async def test_checklist_question_metrics(test_data):

    print()
    report = ChecklistQuestionMetrics()
    metrics_generator = MetricsGenerator(test_data, report)
    metrics_visualizer = MetricsVisualizer(metrics_generator.metrics_table, report.name)
    metrics_visualizer.plot_heatmap()

    print_to_console(metrics_generator.metrics_table, report.name, report.metrics)
    save_to_excel(metrics_generator.metrics_table, os.getenv("EXAMPLES_XLSX_FILE_NAME"), report.name, False, report.metrics)