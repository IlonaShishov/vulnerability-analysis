import pytest
import pandas as pd
import os

from test_lib.data_loader import load_test_data
from test_lib.reports.consistency_metrics import exploitability_consistency_check, justification_label_consistency_check
from test_lib.reports.summary_metrics import SummaryMetrics
from test_lib.reports.justification_label_metrics import JustificationLabelMetrics
from test_lib.reports.checklist_response_metrics import ChecklistResponseMetrics
from test_lib.metrics_generator import MetricsGenerator
from test_lib.utils import print_to_console, save_to_excel

@pytest.fixture
def test_data():
    examples_dir = os.getenv("EXAMPLES_DIR")
    return load_test_data(examples_dir)

def test_consistency_metrics(test_data):

    print ("\nAgent Morpheus Exploitability Consistency Chart")
    exploitability_consistency_chart = exploitability_consistency_check(test_data)
    print_to_console(exploitability_consistency_chart)
    save_to_excel(exploitability_consistency_chart, os.getenv("EXAMPLES_XLSX_FILE_NAME"), "Exploitability Consistency")

    print ("\nAgent Morpheus Justification Label Consistency Chart")
    justification_label_consistency_chart = justification_label_consistency_check(test_data)    
    print_to_console(justification_label_consistency_chart)
    save_to_excel(justification_label_consistency_chart, os.getenv("EXAMPLES_XLSX_FILE_NAME"), "Justification Label Consistency")

@pytest.mark.asyncio
async def test_summary_metrics(test_data):

    print()
    evaluation_item = SummaryMetrics()
    metrics_generator = MetricsGenerator(test_data, evaluation_item)

    print("\nAgent Morpheus Summary Metrics")
    print_to_console(metrics_generator.metrics_table, evaluation_item.metrics)
    save_to_excel(metrics_generator.metrics_table, os.getenv("EXAMPLES_XLSX_FILE_NAME"), "Summary Metrics", False, evaluation_item.metrics)

@pytest.mark.asyncio
async def test_justification_label_metrics(test_data):

    print()
    evaluation_item = JustificationLabelMetrics()
    metrics_generator = MetricsGenerator(test_data, evaluation_item)

    print("\nAgent Morpheus Justification Label Metrics")
    print_to_console(metrics_generator.metrics_table, evaluation_item.metrics)
    save_to_excel(metrics_generator.metrics_table, os.getenv("EXAMPLES_XLSX_FILE_NAME"), "Justification Label Metrics", False, evaluation_item.metrics)

@pytest.mark.asyncio
async def test_checklist_response_metrics(test_data):

    print()
    evaluation_item = ChecklistResponseMetrics()
    metrics_generator = MetricsGenerator(test_data, evaluation_item)

    print("\nAgent Morpheus Checklist Response Metrics")
    print_to_console(metrics_generator.metrics_table, evaluation_item.metrics)
    save_to_excel(metrics_generator.metrics_table, os.getenv("EXAMPLES_XLSX_FILE_NAME"), "Checklist Response Metrics", False, evaluation_item.metrics)